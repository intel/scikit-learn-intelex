<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
  <meta content="This website introduces Intel® Extension for Scikit-learn*" name="description" />
  <meta content="Intel optimization, Scikit-learn, Intel® Extension for Scikit-learn*, GPU, discrete GPU, Intel discrete GPU" name="keywords" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Welcome to Intel® Extension for Scikit-learn* Documentation</title>
  <link rel="stylesheet" href="_css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_css/custom.css" type="text/css" />
  <script src="_scripts/jquery.js"></script>
  <script src="_scripts/theme.js"></script>
</head>

<body class="wy-body-for-nav">
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
          <p id="version-pick"><span><b>Pick a version:</b></span></p>
          <!-- <p><span><b>CPU</b></span></p> -->
          <ul id="ul_cpu">
            <li class="toctree-l1"><a class="reference internal" href="latest">latest</a></li>
            <li class="toctree-l1"><a class="reference internal" href="2023.2">v2023.2</a></li>
          </ul>
          <!-- <p class="menu-separator"></p>
          <p<span><b>XPU/GPU</b></span></p>
          <ul id="ul_gpu">
          </ul> -->
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" aria-label="Mobile navigation menu" >
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="#">intel_extension_for_scikit_learn</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div itemprop="articleBody" >
              <section id="welcome-to-intel-extension-for-scikit-learn-documentation">
                <h1>Welcome to Intel® Extension for Scikit-learn* Documentation<a class="headerlink" href="#welcome-to-intel-extension-for-scikit-learn-documentation" title="Permalink to this heading"></a></h1>
                <p>Scikit-learn* is a Python* module for machine learning. Intel® Extension for Scikit-learn* seamlessly speeds up your scikit-learn applications for Intel CPUs and GPUs across single- and multi-node configurations. This extension package dynamically patches scikit-learn estimators while improving performance for your machine learning algorithms.</p>
                <p>The extension is part of the Intel® AI Analytics Toolkit (AI Kit) that provides flexibility to use machine learning tools with your existing AI packages.</p>
                <p>Using scikit-learn with this extension, you can:</p>
                  <ul>
                    <li>Speed up training and inference by up to 100x with the equivalent mathematical accuracy.</li>
                    <li>Continue to use the open source scikit-learn API.</li>
                    <li>Enable and disable the extension with a couple lines of code or at the command line.</li>
                  </ul>
                <p>You can learn more about Intel® Extension for Scikit-Learn* in the following <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/scikit-learn.html">video</a>.</p>
                <p><b>Designed for Data Scientists and Framework Designers</b></p>
                <p>Intel® Extension for Scikit-learn* was created to provide data scientists with a way to get a better performance while using the familiar scikit-learn package and getting the same results.</p>
                <p>To get started with Intel® Extension for Scikit-Learn*, please review the <a href="latest/gsg.html">Getting Started Guide</a> and <a href="latest/quick-start.html">Quick Start Guide</a> sections of the documentation.</p>
                <!-- <p>Intel® Extension for PyTorch* extends PyTorch* with up-to-date features optimizations for an extra performance boost on Intel hardware. Optimizations take advantage of AVX-512 Vector Neural Network Instructions (AVX512 VNNI) and Intel® Advanced Matrix Extensions (Intel® AMX) on Intel CPUs as well as Intel X<sup>e</sup> Matrix Extensions (XMX) AI engines on Intel discrete GPUs. Moreover, through PyTorch* <cite>xpu</cite> device, Intel® Extension for PyTorch* provides easy GPU acceleration for Intel discrete GPUs with PyTorch*.</p>
                <p>Intel® Extension for PyTorch* provides optimizations for both eager mode and graph mode, however, compared to eager mode, graph mode in PyTorch* normally yields better performance from optimization techniques, such as operation fusion. Intel® Extension for PyTorch* amplifies them with more comprehensive graph optimizations. Therefore we recommend you to take advantage of Intel® Extension for PyTorch* with <a class="reference external" href="https://pytorch.org/docs/stable/jit.html">TorchScript</a> whenever your workload supports it. You could choose to run with <cite>torch.jit.trace()</cite> function or <cite>torch.jit.script()</cite> function, but based on our evaluation, <cite>torch.jit.trace()</cite> supports more workloads so we recommend you to use <cite>torch.jit.trace()</cite> as your first choice.</p>
                <p>The extension can be loaded as a Python module for Python programs or linked as a C++ library for C++ programs. In Python scripts users can enable it dynamically by importing <cite>intel_extension_for_pytorch</cite>.</p>
                <p>Intel® Extension for PyTorch* is structured as shown in the following figure:</p>
                <figure class="align-center">
                  <a class="reference internal image-reference" href="_images/intel_extension_for_pytorch_structure.png"><img alt="Architecture of Intel® Extension for PyTorch*" src="_images/intel_extension_for_pytorch_structure.png" style="width: 800px;" /></a>
                </figure>
                <div class="line-block">
                  <div class="line"><br /></div>
                </div>
                <p>Optimizations for both eager mode and graph mode contribute to extra performance accelerations with the extension. In eager mode, the PyTorch frontend is extended with custom Python modules (such as fusion modules), optimal optimizers, and INT8 quantization APIs. Further performance boost is available by converting the eager-mode model into graph mode via extended graph fusion passes. In the graph mode, the fusions reduce operator/kernel invocation overheads, and thus increase performance. On CPU, Intel® Extension for PyTorch* dispatches the operators into their underlying kernels automatically based on ISA that it detects and leverages vectorization and matrix acceleration units available on Intel hardware. Intel® Extension for PyTorch* runtime extension brings better efficiency with finer-grained thread runtime control and weight sharing. On GPU, optimized operators and kernels are implemented and registered through PyTorch dispatching mechanism. These operators and kernels are accelerated from native vectorization feature and matrix calculation feature of Intel GPU hardware. Intel® Extension for PyTorch* for GPU utilizes the <a class="reference external" href="https://github.com/intel/llvm#oneapi-dpc-compiler">DPC++</a> compiler that supports the latest <a class="reference external" href="https://registry.khronos.org/SYCL/specs/sycl-2020/html/sycl-2020.html">SYCL*</a> standard and also a number of extensions to the SYCL* standard, which can be found in the <a class="reference external" href="https://github.com/intel/llvm/tree/sycl/sycl/doc/extensions">sycl/doc/extensions</a> directory.</p>
                <div class="admonition note">
                  <p class="admonition-title">Note</p>
                  <p>GPU features are not included in CPU only packages.</p>
                </div> 
                <p>Intel® Extension for PyTorch* has been released as an open–source project at <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch">Github</a>. Source code is available at <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/xpu-master">xpu-master branch</a>. Check <a class="reference internal" href="./xpu/latest/">the tutorial</a> for detailed information. Due to different development schedule, optimizations for CPU only might have a newer code base. Source code is available at <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/master">master branch</a>. Check <a class="reference internal" href="./cpu/latest/">the CPU tutorial</a> for detailed information on the CPU side.</p>
                -->
                <div class="toctree-wrapper compound">
                </div> 
              </section> 
            </div>
          </div>

          <footer>
            <hr/>
            <div role="contentinfo">
              <p>&#169; Copyright Intel(R).</p>
            </div>
            Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
            <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
            provided by <a href="https://readthedocs.org">Read the Docs</a>.
          </footer>

        </div>
      </div>
    </section>
  </div>
  <script>
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
    });
  </script>
</body>
</html>
